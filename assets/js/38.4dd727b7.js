(window.webpackJsonp=window.webpackJsonp||[]).push([[38],{636:function(t,e,a){t.exports=a.p+"assets/img/topology.fe80aeb6.jpg"},732:function(t,e,a){"use strict";a.r(e);var s=a(19),n=Object(s.a)({},(function(){var t=this,e=t.$createElement,s=t._self._c||e;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("p",[t._v("The distributed event store and streamlining platform "),s("a",{attrs:{href:"https://kafka.apache.org/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Apache Kafka"),s("OutboundLink")],1),t._v(" have many advantages. Think of high throughput, low latency, durability, high concurrency, scalability, and fault tolerance. So, it's not surprising that when we built "),s("a",{attrs:{href:"https://github.com/kestra-io/kestra",target:"_blank",rel:"noopener noreferrer"}},[t._v("Kestra"),s("OutboundLink")],1),t._v(", an open-source data orchestration and scheduling platform, we decided to use Kafka as the central database to build a scalable architecture. Since Kafka supports high throughput, which allows it to handle high-volume data at equally high velocity, we are confident of a fully scalable solution. However, Kafka has some restrictions since it is not a database, so we need to deal with the constraints and adapt the code to make it work with Kafka.")]),t._v(" "),s("p",[t._v("Since we rely heavily on "),s("a",{attrs:{href:"https://kafka.apache.org/documentation/streams/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Kafka Streams"),s("OutboundLink")],1),t._v(" for most of our services (executor and scheduler), we have made some assumptions on how it handles the workload. This blog post shows some advanced techniques we used to make a Kafka Stream reliable. We want to share the tips we have discovered over the last two years.")]),t._v(" "),s("h2",{attrs:{id:"same-topic-as-source-and-destination"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#same-topic-as-source-and-destination"}},[t._v("#")]),t._v(" Same topic as source and destination")]),t._v(" "),s("p",[t._v("In Kestra, we have a Kafka topic with the current execution, that is the source and also the destination. We only update the current execution to add some information and send it back to Kafka for further processing (New tasks status, for example). We were not sure if this design was possible with Kafka. We "),s("a",{attrs:{href:"https://twitter.com/tchiotludo/status/1252197729406783488",target:"_blank",rel:"noopener noreferrer"}},[t._v("asked"),s("OutboundLink")],1),t._v(" Matthias J. Sax, one of the primary maintainers of Kafka Streams, who responded to me on "),s("a",{attrs:{href:"https://stackoverflow.com/questions/61316312/does-kafka-stream-with-same-sink-source-topics-with-join-is-supported",target:"_blank",rel:"noopener noreferrer"}},[t._v("Stack Overflow"),s("OutboundLink")],1),t._v(".")]),t._v(" "),s("p",[t._v("TL;DR: Yes, it's possible.")]),t._v(" "),s("p",[t._v("Yes, it's possible if you are confident that for the same key (the execution in this"),s("br"),t._v("\n), you have only one process that can write. If you see this warning in the console "),s("code",[t._v("Detected out-of-order KTable update for execution at offset 10, partition 7.")]),t._v(", you likely have more than one process for the same key, which can lead to unexpected behavior (like overwriting previous values).")]),t._v(" "),s("p",[t._v("Imagine a topology with the topic as the source, some branching logic, and two different processes writing to the same destinations:")]),t._v(" "),s("div",{staticClass:"language-java extra-class"},[s("pre",{pre:!0,attrs:{class:"language-java"}},[s("code",[s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("KStream")]),s("span",{pre:!0,attrs:{class:"token generics"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("String")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Execution")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),t._v(" executions "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" builder\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("stream")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"executions"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Consumed")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Serdes"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("String")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("JsonSerde")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("of")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Execution")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\nexecutions\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("mapValues")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("readOnlyKey"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" value"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("->")]),t._v(" value"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("to")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"executions"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Produced")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Serdes"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("String")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("JsonSerde")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("of")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Execution")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\nexecutions\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("leftJoin")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n        builder"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("table")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"results"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Consumed")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Serdes"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("String")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("JsonSerde")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("of")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("WorkerTaskResult")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("readOnlyKey"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" value1"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" value2"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("->")]),t._v(" value1\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("to")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"executions"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Produced")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Serdes"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("String")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("JsonSerde")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("of")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Execution")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),s("p",[t._v("In this case, a concurrent process can write on the same topic, overwriting the previous value and destroying the previously computed methods."),s("br"),t._v("\nFor this kind of usage, you must define a single writer for a Key at a given time. This leads us to our next part, a custom joiner.")]),t._v(" "),s("h2",{attrs:{id:"custom-joiner"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#custom-joiner"}},[t._v("#")]),t._v(" Custom Joiner")]),t._v(" "),s("p",[t._v("To process the execution, we have separated the microservice into multiple topics. The more straightforward topology to understand is:")]),t._v(" "),s("ul",[s("li",[t._v("a topic with the executions (with multiple tasks)")]),t._v(" "),s("li",[t._v("a topic with task results")])]),t._v(" "),s("p",[t._v("To allow the next task to start, we need to create a state with all task results merged into the current execution. The first thought was to use "),s("code",[t._v("join()")]),t._v(" from Kafka Streams. What a terrible choice! ðŸ˜‰")]),t._v(" "),s("p",[t._v("All joins provided by Kafka Streams were designed with aggregation in mind, like sum, avg, etc. It works well for that case. It will simply process all the incoming data from both topics 1 per 1. We will see all the changes on the streams on both sides, as seen below:")]),t._v(" "),s("div",{staticClass:"language- extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("# timeline\n--A-------- > Execution\n-----B--C-- > Task Result\n\n# join result timeline\n- (A,null)\n- (A, B) => emit (A+B)\n- (A, C) => emit (A+C) <=== you have overwrite the result of A+B\n- (A+B, null)\n- (A+C, null) <== we will never have (A+B+C)\n")])])]),s("p",[t._v("In our case, we are building a state machine and want to keep the last state of execution, meaning we do not want to see the intermediate states. In this case, we have no choice but to build a custom joiner since Kafka Streams doesn't have one built-in.")]),t._v(" "),s("p",[t._v("Our custom joiner needs to:")]),t._v(" "),s("ul",[s("li",[s("a",{attrs:{href:"https://github.com/kestra-io/kestra/blob/develop/runner-kafka/src/main/java/io/kestra/runner/kafka/streams/ExecutorFromExecutionTransformer.java",target:"_blank",rel:"noopener noreferrer"}},[t._v("Manually create a store"),s("OutboundLink")],1),t._v(" that will save the last state of an execution.")]),t._v(" "),s("li",[t._v("Create a custom "),s("a",{attrs:{href:"https://github.com/kestra-io/kestra/blob/8a17519b5b41e8ba0ab8e0044c9c6b1e887ccd94/runner-kafka/src/main/java/io/kestra/runner/kafka/executors/ExecutorMain.java#L216-L246",target:"_blank",rel:"noopener noreferrer"}},[t._v("merge function"),s("OutboundLink")],1),t._v(" that will merge the execution stream with the task results stream.")]),t._v(" "),s("li",[s("a",{attrs:{href:"https://github.com/kestra-io/kestra/blob/develop/runner-kafka/src/main/java/io/kestra/runner/kafka/streams/ExecutorJoinerTransformer.java",target:"_blank",rel:"noopener noreferrer"}},[t._v("Get the last value"),s("OutboundLink")],1),t._v(" from the state, add the task result and emit the new state that will finally be saved on the store and the final topics.")])]),t._v(" "),s("p",[t._v("With all this, we are sure that the execution state will always be the last version, whatever the number of tasks results coming in parallel might be.")]),t._v(" "),s("h2",{attrs:{id:"easy-distributed-workload-between-multiple-backends"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#easy-distributed-workload-between-multiple-backends"}},[t._v("#")]),t._v(" Easy distributed workload between multiple backends")]),t._v(" "),s("p",[t._v("In "),s("a",{attrs:{href:"https://github.com/kestra-io/kestra",target:"_blank",rel:"noopener noreferrer"}},[t._v("Kestra"),s("OutboundLink")],1),t._v(", we have a scheduler that will either look up all flows to schedule execution (timed-based) or with a long-polling mechanism (detecting files on S3 or SFTP). As we wanted to avoid a single point of failure on this service, we needed to split the flow between all instances of schedulers.")]),t._v(" "),s("p",[t._v("To handle that case, we rely on Kafka's consumer groups that will handle the complexity of a distributed system for us. Here's the logic:")]),t._v(" "),s("ul",[s("li",[t._v("Create a "),s("a",{attrs:{href:"https://github.com/kestra-io/kestra/blob/develop/runner-kafka/src/main/java/io/kestra/runner/kafka/KafkaFlowListeners.java",target:"_blank",rel:"noopener noreferrer"}},[t._v("Kafka stream"),s("OutboundLink")],1),t._v(" that will read in a "),s("code",[t._v("KTable")]),t._v(" and transmit all the results to a "),s("code",[t._v("Consumer")]),t._v(".")]),t._v(" "),s("li",[t._v("Listen to state change, meaning mostly "),s("code",[t._v("REBALANCED")]),t._v(" streams, and empty all the flows for the "),s("code",[t._v("Consumer")]),t._v(".")]),t._v(" "),s("li",[t._v("On the "),s("code",[t._v("READY")]),t._v(" state, read all the "),s("code",[t._v("KTable")]),t._v(" again.")])]),t._v(" "),s("p",[t._v("With these, all flows will be dispatched to all listeners. That means if you have a thousand flows, every consumer will have ~500 flows (depending on the repartition of keys). Kafka will handle all the heavy parts of the distributed systems, such as:")]),t._v(" "),s("ul",[s("li",[t._v("Heartbeat to detect failure for a consumer")]),t._v(" "),s("li",[t._v("Notifying of rebalancing")]),t._v(" "),s("li",[t._v("Ensure exactly-once pattern for a topic, ensuring that only one consumer will handle the data")])]),t._v(" "),s("p",[t._v("This way, you will have a fully distributed system thanks to Kafka without the pain of passing a Jespen analysis.")]),t._v(" "),s("h2",{attrs:{id:"use-partitions-to-detect-dead-consumers"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#use-partitions-to-detect-dead-consumers"}},[t._v("#")]),t._v(" Use partitions to detect dead consumers")]),t._v(" "),s("p",[t._v("For Kestra, we need to detect when a worker was processing a task and died. The reasons for this could range from an outage to a simple restart during processing.")]),t._v(" "),s("p",[t._v("Thanks to the Kafka consumer, we can know the partition affected by this consumer. We use these excellent features to detect dead workers:")]),t._v(" "),s("ul",[s("li",[t._v("We create a "),s("code",[t._v("UUID")]),t._v(" on startup for the worker.")]),t._v(" "),s("li",[t._v("When a consumer connects to Kafka, we "),s("a",{attrs:{href:"https://github.com/kestra-io/kestra/blob/4a61af45d668feab19313b9033826fa7075bf02b/runner-kafka/src/main/java/io/kestra/runner/kafka/KafkaWorkerTaskQueue.java#L157-L187",target:"_blank",rel:"noopener noreferrer"}},[t._v("listen to the partitions"),s("OutboundLink")],1),t._v(" affected using a "),s("code",[t._v("ConsumerRebalanceListener")]),t._v(". We publish to Kafka a WorkerInstance with "),s("code",[t._v("UUID")]),t._v(" and assigned partition.")]),t._v(" "),s("li",[t._v("For each task run, we publish a message with "),s("a",{attrs:{href:"https://github.com/kestra-io/kestra/blob/4a61af45d668feab19313b9033826fa7075bf02b/runner-kafka/src/main/java/io/kestra/runner/kafka/KafkaWorkerTaskQueue.java#L112-L123",target:"_blank",rel:"noopener noreferrer"}},[t._v("TaskRunning"),s("OutboundLink")],1),t._v(" with the worker UUID.")])]),t._v(" "),s("p",[t._v("Now, let's handle the data stored in Kafka. The main logic is a "),s("a",{attrs:{href:"https://github.com/kestra-io/kestra/blob/develop/runner-kafka/src/main/java/io/kestra/runner/kafka/streams/WorkerInstanceTransformer.java",target:"_blank",rel:"noopener noreferrer"}},[t._v("Kafka Stream"),s("OutboundLink")],1),t._v(" which will:")]),t._v(" "),s("ul",[s("li",[t._v("Create a global "),s("code",[t._v("KTable")]),t._v(" with all the "),s("code",[t._v("WorkerInstance")]),t._v(".")]),t._v(" "),s("li",[t._v("On every change, it will listen to the changed "),s("code",[t._v("WorkerInstance")]),t._v(".")]),t._v(" "),s("li",[t._v("If there is a new "),s("code",[t._v("WorkerInstance")]),t._v(", we look at the partition assigned to this one. If there is an overlap between this instance's partitions and the previous one, we know that the previous "),s("code",[t._v("WorkerInstance")]),t._v(" is dead. In Kafka, you can't have two consumers on the same partition.")]),t._v(" "),s("li",[t._v("We only need to look at the affected tasks in this "),s("code",[t._v("WorkerInstance")]),t._v(" and resend them for processing.")])]),t._v(" "),s("p",[t._v("Et voila! We have detection of dead consumers using just the Kafka API. ðŸŽ‰")]),t._v(" "),s("h2",{attrs:{id:"state-store-all"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#state-store-all"}},[t._v("#")]),t._v(" State store "),s("code",[t._v("all()")])]),t._v(" "),s("p",[t._v("We use a "),s("a",{attrs:{href:"https://kafka.apache.org/31/documentation/streams/developer-guide/dsl-api.html#streams_concepts_globalktable",target:"_blank",rel:"noopener noreferrer"}},[t._v("GlobalKTable"),s("OutboundLink")],1),t._v(" to detect "),s("RouterLink",{attrs:{to:"/docs/developer-guide/triggers/flow.html"}},[t._v("flow trigger")]),t._v(". For all the flows on the cluster, we test all the flow's "),s("RouterLink",{attrs:{to:"/docs/developer-guide/conditions/"}},[t._v("conditions")]),t._v(" to find matching flows. For this, we are using an API to fetch all flows from a "),s("code",[t._v("GlobalKTable")]),t._v(" using "),s("code",[t._v("store.all()")]),t._v(" that returns all the flows, fetching RocksDB.")],1),t._v(" "),s("p",[t._v("Our first assumption was "),s("code",[t._v("all()")]),t._v(" returns an object (Flow in our case), as the API return Object, but we discovered that the "),s("code",[t._v("all()")]),t._v(" will:")]),t._v(" "),s("ul",[s("li",[t._v("Fetch all the data from RocksDB (ok for that)")]),t._v(" "),s("li",[t._v("Deserialize the data from RocksDB that is stored as byte and map it to concrete Java POJO")])]),t._v(" "),s("p",[t._v("So each time we call "),s("code",[t._v("all()")]),t._v(" on the method, all values are deserialized, which can lead to high CPU usage and latency on your stream. We are talking about all flows revision on our cluster for our use case. 2.5K flows (last revision), but we don't see people creating a lot of revisions (100K). Imagine 100K "),s("code",[t._v("byte[]")]),t._v(" to deserialize to POJO for every call (mostly all the execution end).")]),t._v(" "),s("p",[t._v("Since we only need the last revision in our use case, we create an in-memory Map with all the flows using the following:")]),t._v(" "),s("div",{staticClass:"language-java extra-class"},[s("pre",{pre:!0,attrs:{class:"language-java"}},[s("code",[t._v("builder"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("addGlobalStore")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Stores")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("keyValueStoreBuilder")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Stores")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("persistentKeyValueStore")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("FLOW_STATE_STORE_NAME"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Serdes"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("String")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("JsonSerde")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("of")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Flow")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    kafkaAdminService"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("getTopicName")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Flow")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Consumed")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("with")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Serdes"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("String")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("JsonSerde")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("of")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Flow")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("class")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("withName")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"GlobalStore.Flow"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("->")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("GlobalInMemoryStateProcessor")]),s("span",{pre:!0,attrs:{class:"token generics"}},[s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("<")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(">")])]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n        FLOW_STATE_STORE_NAME"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        flows "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("->")]),t._v(" kafkaFlowExecutor"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("setFlows")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("flows"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        store "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("->")]),t._v(" kafkaFlowExecutor"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("setStore")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("store"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),s("p",[s("a",{attrs:{href:"https://github.com/kestra-io/kestra/blob/master/runner-kafka/src/main/java/io/kestra/runner/kafka/streams/GlobalInMemoryStateProcessor.java",target:"_blank",rel:"noopener noreferrer"}},[t._v("GlobalInMemoryStateProcessor"),s("OutboundLink")],1),t._v(" is a simple wrapper that saves to state store and sends a complete list on every change (not so frequent). Using this, we decided to gather all the flows in memory. This works well for our use cases because we know that an instance of Kestra will not have millions of flows.")]),t._v(" "),s("p",[t._v("Remember that all store operations (like get) will lead to deserialization that costs you some CPU.")]),t._v(" "),s("h2",{attrs:{id:"many-source-topics-within-a-stream"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#many-source-topics-within-a-stream"}},[t._v("#")]),t._v(" Many source topics within a stream")]),t._v(" "),s("p",[t._v("At first, we designed Kestra to have only one "),s("strong",[t._v("huge")]),t._v(" stream for all the processing from the executor. At first, it seemed cool, but this led to some drawbacks.")]),t._v(" "),s("p",[t._v("Here is the last version of our main and only Kafka Stream with many topics ðŸ™‰:"),s("br"),t._v(" "),s("img",{attrs:{src:a(636),alt:"Kestra Topology"}}),s("br"),t._v("\nYes, this is a huge Kafka Stream. It was working almost despite the complexity of this one. But the major drawbacks were :")]),t._v(" "),s("ul",[s("li",[s("strong",[t._v("Monitoring")]),t._v(": All the metrics are under the same consumer groups")]),t._v(" "),s("li",[s("strong",[t._v("Debugging")]),t._v(": Each topic is consumed independently during a crash. When a message fails, the whole process crashes.")]),t._v(" "),s("li",[s("strong",[t._v("Lag")]),t._v(": This is the most important one. Since Kafka Streams optimize the consumption of messages by themselves, a topic with large outputs could lead to lag on unrelated topics. In that case, it is impossible to understand the lag on our consumers.")])]),t._v(" "),s("p",[t._v("Now, we have decided to split into multiple "),s("a",{attrs:{href:"https://github.com/kestra-io/kestra/tree/develop/runner-kafka/src/main/java/io/kestra/runner/kafka/executors",target:"_blank",rel:"noopener noreferrer"}},[t._v("streams"),s("OutboundLink")],1),t._v(" to be able to monitor and properly understand the lag on our Kafka Streams.")]),t._v(" "),s("p",[t._v("How to split your giant stream with lots of topics (evil streams)? We consumed only one topic at a time (to avoid large network transit), so we grouped all streams by source topics.")]),t._v(" "),s("h2",{attrs:{id:"conclusion"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#conclusion"}},[t._v("#")]),t._v(" Conclusion")]),t._v(" "),s("p",[t._v("We have covered some tips that took us a lot of time to find how to deal with the issues. Kestra is the only infinitely scalable data orchestration and scheduling platform that uses Kafka as the backend and allows millions of executions.")]),t._v(" "),s("p",[t._v("We hope you have enjoyed this one. Stay connected and follow Kestra on "),s("a",{attrs:{href:"https://github.com/kestra-io/kestra",target:"_blank",rel:"noopener noreferrer"}},[t._v("GitHub"),s("OutboundLink")],1),t._v(", "),s("a",{attrs:{href:"https://twitter.com/kestra_io",target:"_blank",rel:"noopener noreferrer"}},[t._v("Twitter"),s("OutboundLink")],1),t._v(", or "),s("a",{attrs:{href:"https://api.kestra.io/v1/communities/slack/redirect",target:"_blank",rel:"noopener noreferrer"}},[t._v("Slack"),s("OutboundLink")],1),t._v(".")])])}),[],!1,null,null,null);e.default=n.exports}}]);